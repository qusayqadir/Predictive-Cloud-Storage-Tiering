{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modelling - Determining AWS S3 Storage Tier\n",
    "\n",
    "Use the syenthetic data to build model(s) that predicits the storage tier of new files. \n",
    "To efficiently and correctly catergorize files into different storage tiers is useful becuase it allows for cost optimization, storage efficiency, and client performance optimization: retrivel times "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS S3 Tiers \n",
    "\n",
    "The model(s) built will analyze the metadata of the file to store it in one of three storage classes\n",
    "\n",
    "Overview of the storage classses in use: \n",
    "\n",
    "<table align=\"left\" style=\"width:50%\"> \n",
    "    <tr>\n",
    "        <th>Class</th>\n",
    "        <th>Use Case</th>\n",
    "        <th>Tier</th> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>S3 Standard</td>\n",
    "        <td>Frequently Accessed</td>\n",
    "        <td>\"Hot\"</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>S3 One Zone-IA</td>\n",
    "        <td>Infrequent, low-availability data </td>\n",
    "        <td>\"Warm\"</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>S3 Glacier (Deep Archieve?)</td>\n",
    "        <td>Rarely Accessed / Long-term rarely accessed</td>\n",
    "        <td>\"Cold\"</td>\n",
    "    </tr>\n",
    "</table> \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1) Data Preperation - preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible approaches towards a dataset preprocessing before fitting it a model \n",
    "\n",
    "**1)** Handle missing/null values \n",
    "\n",
    "**2)** Normalize/scale numerical features \n",
    "\n",
    "Standardization: Scale values to have a mean of 0 and standard deviation of 1.\n",
    "\n",
    "Normalization: Rescale values to fall within a range (e.g., 0â€“1).\n",
    "\n",
    "**3)** Encode categorical features : ordinal encoding ( hot > warm > cold)\n",
    "\n",
    "**4)** Feature engineering + Class imbalance ( create/remove columns ) \n",
    "\n",
    "New rows for edge cases to help the model learn critical boundries,\n",
    "\n",
    "Talk about not dropping access_frequency and frequency of access.  \n",
    "\n",
    "need to determine if the target variable 'Storage_Tier' in the dataset is balanced, risk the model becoming biased  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing/null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_ID</th>\n",
       "      <th>Access_Frequency</th>\n",
       "      <th>Frequnecy_of_Access</th>\n",
       "      <th>File_Size</th>\n",
       "      <th>File_Lifecycle_Stage</th>\n",
       "      <th>Modification_Frequency</th>\n",
       "      <th>File_Age</th>\n",
       "      <th>Storage_Tier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>File_1</td>\n",
       "      <td>21.2307</td>\n",
       "      <td>24.4205</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>23.3177</td>\n",
       "      <td>2.6015</td>\n",
       "      <td>Hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>File_2</td>\n",
       "      <td>24.0589</td>\n",
       "      <td>4.5884</td>\n",
       "      <td>1.8784</td>\n",
       "      <td>8.4383</td>\n",
       "      <td>4.8516</td>\n",
       "      <td>7.7843</td>\n",
       "      <td>Warm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>File_3</td>\n",
       "      <td>25.1752</td>\n",
       "      <td>10.1110</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.7295</td>\n",
       "      <td>1.9126</td>\n",
       "      <td>Warm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>File_4</td>\n",
       "      <td>4.2926</td>\n",
       "      <td>17.4557</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.7832</td>\n",
       "      <td>2.3784</td>\n",
       "      <td>Cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>File_5</td>\n",
       "      <td>19.9357</td>\n",
       "      <td>23.3908</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>12.2302</td>\n",
       "      <td>3.5120</td>\n",
       "      <td>Warm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>File_6</td>\n",
       "      <td>28.4249</td>\n",
       "      <td>23.1148</td>\n",
       "      <td>7.0560</td>\n",
       "      <td>9.1548</td>\n",
       "      <td>9.2075</td>\n",
       "      <td>1.8900</td>\n",
       "      <td>Hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>File_7</td>\n",
       "      <td>19.6673</td>\n",
       "      <td>8.6535</td>\n",
       "      <td>6.2278</td>\n",
       "      <td>7.9220</td>\n",
       "      <td>14.8424</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Warm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>File_8</td>\n",
       "      <td>16.5561</td>\n",
       "      <td>22.7516</td>\n",
       "      <td>4.0345</td>\n",
       "      <td>3.5290</td>\n",
       "      <td>9.6637</td>\n",
       "      <td>5.4443</td>\n",
       "      <td>Warm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>File_9</td>\n",
       "      <td>9.0017</td>\n",
       "      <td>8.5943</td>\n",
       "      <td>15.8221</td>\n",
       "      <td>15.2384</td>\n",
       "      <td>9.1963</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Warm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>File_10</td>\n",
       "      <td>17.6529</td>\n",
       "      <td>18.1427</td>\n",
       "      <td>10.5044</td>\n",
       "      <td>4.3349</td>\n",
       "      <td>4.9068</td>\n",
       "      <td>5.5250</td>\n",
       "      <td>Warm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   File_ID  Access_Frequency  Frequnecy_of_Access  File_Size  \\\n",
       "0   File_1           21.2307              24.4205     0.0000   \n",
       "1   File_2           24.0589               4.5884     1.8784   \n",
       "2   File_3           25.1752              10.1110     0.0000   \n",
       "3   File_4            4.2926              17.4557     0.0000   \n",
       "4   File_5           19.9357              23.3908     0.0000   \n",
       "5   File_6           28.4249              23.1148     7.0560   \n",
       "6   File_7           19.6673               8.6535     6.2278   \n",
       "7   File_8           16.5561              22.7516     4.0345   \n",
       "8   File_9            9.0017               8.5943    15.8221   \n",
       "9  File_10           17.6529              18.1427    10.5044   \n",
       "\n",
       "   File_Lifecycle_Stage  Modification_Frequency  File_Age Storage_Tier  \n",
       "0                0.0000                 23.3177    2.6015          Hot  \n",
       "1                8.4383                  4.8516    7.7843         Warm  \n",
       "2                0.0000                  5.7295    1.9126         Warm  \n",
       "3                0.0000                  8.7832    2.3784         Cold  \n",
       "4                0.0000                 12.2302    3.5120         Warm  \n",
       "5                9.1548                  9.2075    1.8900          Hot  \n",
       "6                7.9220                 14.8424    0.0000         Warm  \n",
       "7                3.5290                  9.6637    5.4443         Warm  \n",
       "8               15.2384                  9.1963    0.0000         Warm  \n",
       "9                4.3349                  4.9068    5.5250         Warm  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "np.random.seed(1)  \n",
    "\n",
    "df = pd.read_csv(\"../../data/train-model/train-file-metadata.csv\") \n",
    "\n",
    "df.head(n=10) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   File_ID                 2000 non-null   object \n",
      " 1   Access_Frequency        2000 non-null   float64\n",
      " 2   Frequnecy_of_Access     2000 non-null   float64\n",
      " 3   File_Size               2000 non-null   float64\n",
      " 4   File_Lifecycle_Stage    2000 non-null   float64\n",
      " 5   Modification_Frequency  2000 non-null   float64\n",
      " 6   File_Age                2000 non-null   float64\n",
      " 7   Storage_Tier            2000 non-null   object \n",
      "dtypes: float64(6), object(2)\n",
      "memory usage: 125.1+ KB\n",
      "None\n",
      "['Hot' 'Warm' 'Cold']\n",
      "(2000, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Access_Frequency</th>\n",
       "      <th>Frequnecy_of_Access</th>\n",
       "      <th>File_Size</th>\n",
       "      <th>File_Lifecycle_Stage</th>\n",
       "      <th>Modification_Frequency</th>\n",
       "      <th>File_Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15.515501</td>\n",
       "      <td>12.030583</td>\n",
       "      <td>7.976287</td>\n",
       "      <td>5.460082</td>\n",
       "      <td>10.834073</td>\n",
       "      <td>2.669299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.291801</td>\n",
       "      <td>7.569856</td>\n",
       "      <td>8.205440</td>\n",
       "      <td>5.579764</td>\n",
       "      <td>6.968456</td>\n",
       "      <td>2.702448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.077500</td>\n",
       "      <td>1.007200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.013700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.705800</td>\n",
       "      <td>5.658150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.471100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14.539200</td>\n",
       "      <td>9.676650</td>\n",
       "      <td>6.135600</td>\n",
       "      <td>4.304750</td>\n",
       "      <td>8.902350</td>\n",
       "      <td>2.147300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>21.577300</td>\n",
       "      <td>18.057800</td>\n",
       "      <td>13.035925</td>\n",
       "      <td>8.843775</td>\n",
       "      <td>15.534425</td>\n",
       "      <td>4.273350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>34.595800</td>\n",
       "      <td>29.673700</td>\n",
       "      <td>29.696700</td>\n",
       "      <td>19.798900</td>\n",
       "      <td>29.658500</td>\n",
       "      <td>9.889600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Access_Frequency  Frequnecy_of_Access    File_Size  \\\n",
       "count       2000.000000          2000.000000  2000.000000   \n",
       "mean          15.515501            12.030583     7.976287   \n",
       "std            8.291801             7.569856     8.205440   \n",
       "min            2.077500             1.007200     0.000000   \n",
       "25%            8.705800             5.658150     0.000000   \n",
       "50%           14.539200             9.676650     6.135600   \n",
       "75%           21.577300            18.057800    13.035925   \n",
       "max           34.595800            29.673700    29.696700   \n",
       "\n",
       "       File_Lifecycle_Stage  Modification_Frequency     File_Age  \n",
       "count           2000.000000             2000.000000  2000.000000  \n",
       "mean               5.460082               10.834073     2.669299  \n",
       "std                5.579764                6.968456     2.702448  \n",
       "min                0.000000                1.013700     0.000000  \n",
       "25%                0.000000                5.471100     0.000000  \n",
       "50%                4.304750                8.902350     2.147300  \n",
       "75%                8.843775               15.534425     4.273350  \n",
       "max               19.798900               29.658500     9.889600  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df[\"Storage_Tier\"].unique())\n",
    "print(df.shape) \n",
    "\n",
    "\n",
    "df.isna().sum() # shows that there is a complete dataset no missing or null values \n",
    "df.describe() #  column values needs to be normalized for better model fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the description of the dataset shows that the values need to be standardized for all the columns to have a mean of 0 and a standard deviation of 1. also shows evidence for normalizatoin so will need to resacle values to fall under the range 0 - 1\n",
    "\n",
    "the dataset is complete, no handling of missing values that have to be imputed by the mean of certain columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize & Standardize columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why is it important to standardize and normalize the values? \n",
    "\n",
    "standardize helps avoid the features that have larger values from dominating the model and developing a bias and helps bring the values closer to a normal distribution ( i will use visualization tools to check if this is true - seaborn? ). normalizing helps all the features be scaled to the same range most commonly 0 - 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler \n",
    "\n",
    "\n",
    "stand__norm_columns = df.select_dtypes(include=['float64']).columns\n",
    "\n",
    "scaler_standard = StandardScaler() \n",
    "scaler_minmaxScaler = MinMaxScaler() \n",
    "\n",
    "df_copy = df.copy() \n",
    "\n",
    "#Standardize\n",
    "standardized_value = scaler_standard.fit_transform(df[stand__norm_columns])\n",
    "for row, col in enumerate(stand__norm_columns): \n",
    "    df_copy[f\"{col}\"] = standardized_value[:, row ]\n",
    "\n",
    " \n",
    "#Normalzie \n",
    "normalized_values = scaler_minmaxScaler.fit_transform(df[stand__norm_columns])\n",
    "for row, col in enumerate(stand__norm_columns): \n",
    "    df_copy[f\"{col}\"] = normalized_values[:, row]\n",
    "\n",
    "df_copy = df_copy.round(4) # higher precision for imporved accuracy \n",
    "\n",
    "np.random.seed(2) \n",
    "\n",
    "output_file = \"preprocessed-train-file-metadata.csv\" \n",
    "df_copy.to_csv(output_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, use the 'preprocessed-train-file-metadata.csv' for further preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Categorical Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "  File_ID  Access_Frequency  Frequnecy_of_Access  File_Size  \\\n",
      "0  File_1            0.5890               0.8167     0.0000   \n",
      "1  File_2            0.6760               0.1249     0.0633   \n",
      "2  File_3            0.7103               0.3176     0.0000   \n",
      "\n",
      "   File_Lifecycle_Stage  Modification_Frequency  File_Age Storage_Tier  \\\n",
      "0                0.0000                  0.7786    0.2631          Hot   \n",
      "1                0.4262                  0.1340    0.7871         Warm   \n",
      "2                0.0000                  0.1646    0.1934         Warm   \n",
      "\n",
      "   Storage_Tier_Encoded  \n",
      "0                     2  \n",
      "1                     1  \n",
      "2                     1  \n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"preprocessed-train-file-metadata.csv\") \n",
    "\n",
    "df.head(n=5)\n",
    "\n",
    "df.columns.unique() \n",
    "\n",
    "print(df[\"Storage_Tier\"].dtype)  \n",
    "\n",
    "tier_mapping = { \"Hot\" : 2 , \"Warm\" : 1 , \"Cold\" : 0 }\n",
    "\n",
    "df[\"Storage_Tier_Encoded\"] = df[\"Storage_Tier\"].map(tier_mapping) \n",
    "output_file = (\"preprocessed-train-file-metadata.csv\" )\n",
    "df.to_csv(output_file, index=False) \n",
    "\n",
    "print(df.head(n=3))\n",
    "\n",
    "print(df[\"Storage_Tier_Encoded\"].dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "used ordinal coding becuase the value of storage tier does matter to train the model\n",
    "\n",
    "hot > warm > cold \n",
    "\n",
    "2 > 1 > 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering \n",
    "\n",
    "will drop the storage tier  string column , obvious reason \n",
    "\n",
    "try to deteremine if each of the columns have a normal distribution, check for tight edge cases that will imporve the model for boundary cases: if this is lacking then will manually add more rows \n",
    "\n",
    "check if there is a class imbalance for the storage tier to avoid the model overfitting on a certain tier and having a bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) Baseline Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitc Regresion Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3) Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  K-nearest neighbors (KNN) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Boosting (XGBoost) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4) Model Deployment "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
